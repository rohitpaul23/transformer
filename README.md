# transformer

In this project:
1. Implemented the code neccesary for Bidirectional Encoder Representation from Transformer (BERT).
2. Understand how the C4 dataset is structured.
3. Use a pretrained model for inference.
4. Understand how the "Text to Text Transfer from Transformers" or T5 model works.

Created two model in action in two file:
1. BERT_Loss_Model - Using pre-trained model to predict the mask word
2. T5_SQuAD_Model - Using pre-trained model to answer a question based on a given context
